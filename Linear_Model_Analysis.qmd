---
title: "Deepka_Gugulla_652_project"
author: "Deepak_Gugulla_Stat652_project"
format: 
  html:
    self-contained: true
---

```{r}

library(readr)      
library(vip) 
library(tidymodels)
library(readr)
```

```{r}
# load data set
hotels <- 
  read_csv("https://tidymodels.org/start/case-study/hotels.csv") %>%
  mutate(across(where(is.character), as.factor))
```

### Data Splitting and Resampling

```{r}
# Splitting the data set into training and testing data set
set.seed(123)
splits      <- initial_split(hotels, strata = children)

hotel_other <- training(splits)
hotel_test  <- testing(splits)

# training set proportions by children
hotel_other %>% 
  count(children) %>% 
  mutate(prop = n/sum(n))


# test set proportions by children
hotel_test  %>% 
  count(children) %>% 
  mutate(prop = n/sum(n))

```

```{r}

set.seed(234)
val_set <- validation_split(hotel_other, 
                            strata = children, 
                            prop = 0.80)
val_set

```

### Q1. Build the PENALIZED LOGISTIC REGRESSION model the hotel data. In this case study, explain how the recipe and workflow functions are used to prepare the data for the model. Also, explain how the tune_grid is used.

\***ANS** :

```{r}
# Loading the package
library(glmnet)
# Setting up the logistic regression model
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```

### Creating the recipe

Categorical variables are converted to dummy variables and then scaling the variables that is removing zero-variance predictors and Normalizing predictors.

```{r}
#vector for holidays
holidays <- c("AllSouls", "AshWednesday", "ChristmasEve", "Easter", 
              "ChristmasDay", "GoodFriday", "NewYearsDay", "PalmSunday")

# data preprocessing 
lr_recipe <- 
  recipe(children ~ ., data = hotel_other) %>% 
# To consider 'arrival_date' variable as a date
  step_date(arrival_date) %>% 
  step_holiday(arrival_date, holidays = holidays) %>% 
  step_rm(arrival_date) %>% 
# categorical predictors to dummy variables
  step_dummy(all_nominal_predictors()) %>% 
# Removing predictors with zero variance
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

```

### Creating the workflow

```{r}
# workflow() is used to initialize a workflow, the add_model() is used to add the logistic regression and add_recipe() to add the recipe
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)
```

### Creating the grid for tuning

```{r}
# tibble() is used for creating a data frame to store a sequence of penalty values
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

# Lowest penalty values
lr_reg_grid %>% top_n(-5) 

# Highest penalty values
lr_reg_grid %>% top_n(5)  

```

### Hyper-parameter Tuning with tune_grid :

Hyperparameter tuning involved using the tune_grid function on the validation set with varying penalty values for the logistic regression model.

During this process, tune_grid systematically explores the specified hyperparameter grid (lr_reg_grid), trains models with different combinations of hyperparameters on the validation set (val_set), and assesses their performance using the designated metric (roc_auc).

The goal of this procedure is to optimize the penalty parameter, thereby improving the model's ability to generalize well to unseen data.

```{r}

lr_res <- 
  lr_workflow %>% 
  tune_grid(val_set,
# Specifying the grid of penalty values to be tested during tuning            
            grid = lr_reg_grid,
# predictions on the validation set should be saved during tuning
            control = control_grid(save_pred = TRUE),
# Specifying the performance metric to be used for evaluating models during tuning
            metrics = metric_set(roc_auc))
```

### Plotting ROC curve

```{r}

lr_plot <- 
  lr_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())

lr_plot 

```

### Checking model's performance

```{r}

top_models <-
  lr_res %>% 
  show_best("roc_auc", n = 15) %>% 
  arrange(penalty) 
top_models
```

### visualizing the validation set ROC curve

```{r}
lr_best <- 
  lr_res %>% 
  collect_metrics() %>% 
  arrange(penalty) %>% 
  slice(12)
lr_best
```

```{r}
# Plotting the ROC curve for the best logistic regression model
lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)
```

### Q2. Build the TREE-BASED ENSEMBLE model the hotel data.

\***Ans** :

For the hotel dataset, we chose to construct a tree-based ensemble model using the random forest algorithm. This was implemented through the rand_forest function, utilizing the ranger engine to leverage parallel processing capabilities and enhance computational efficiency.

By integrating the model into our workflow and following the same preprocessing steps as previously outlined, we aimed for consistency and coherence in our approach.

To optimize the model's performance, we focused on fine-tuning the "mtry" and "min_n" hyperparameters. This involved utilizing the tune_grid function with a predefined grid size to explore various combinations of these parameters.

The effectiveness of each configuration was evaluated using ROC AUC metrics, allowing us to identify the most optimal parameter settings for achieving the best model performance.

```{r}
cores <- parallel::detectCores()
cores
#> [1] 10
```

```{r}
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")
```

```{r}
# Creating the recipe
rf_recipe <- 
  recipe(children ~ ., data = hotel_other) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date) %>% 
  step_rm(arrival_date) 
```

```{r}
# Creating the workflow
rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)
```

```{r}
rf_mod

# show what will be tuned
extract_parameter_set_dials(rf_mod)

```

```{r}
set.seed(345)
rf_res <- 
  rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
#> i Creating pre-processing data to finalize unknown parameter: mtry
```

```{r}
# Identifying the Random Forest model that performs the best in predicting the outcome of interest, such as whether hotels have children staying or not
rf_res %>% 
  show_best(metric = "roc_auc")

```

```{r}

autoplot(rf_res)
```

```{r}
# Identifying the Random Forest model that performs the best in predicting the outcome of interest
rf_best <- 
  rf_res %>% 
  select_best(metric = "roc_auc")
rf_best
#> # A tibble: 1 Ã— 3
#>    mtry min_n .config              
#>   <int> <int> <chr>                
#> 1     8     7 Preprocessor1_Model13
```

```{r}
# Displaying the predictions made by each model in the Random Forest tuning process
rf_res %>% 
  collect_predictions()
```

```{r}
# ROC curve for the best-performing Random Forest model obtained from the tuning process
rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Random Forest")
```

### Q3. Compare the ROC Curve for the two models and explain which model is better for classifying a hotel booking as with children or no children.

\***ANS** :

Upon analyzing the ROC curves for both models, it was evident that the Random Forest model exhibited a more pronounced bend in the top-left corner. Following an extensive tuning and evaluation process, this observation underscored the Random Forest approach's superior performance.

In essence, the conclusion indicates that the Random Forest model outperforms the alternative model in accurately categorizing hotel reservations according to the presence of children.

```{r}

bind_rows(rf_auc, lr_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)
```

```{r}
 

# the last model
last_rf_mod <- 
  rand_forest(mtry = 8, min_n = 7, trees = 1000) %>% 
  set_engine("ranger", num.threads = cores, importance = "impurity") %>% 
  set_mode("classification")

# the last workflow
last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit
set.seed(345)
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(splits)

last_rf_fit

```

```{r}

last_rf_fit %>% 
  collect_metrics()
```

```{r}

last_rf_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20)
```

```{r}
# Visualization of the ROC curve for the last fitted Random Forest model
last_rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(children, .pred_children) %>% 
  autoplot()
```
